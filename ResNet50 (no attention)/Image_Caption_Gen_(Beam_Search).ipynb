{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision ultralytics nltk pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVSwHxhtrtQq",
        "outputId": "69a88d04-b950-4074-f6ee-2f0e0dde1eae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5PMBDCZr0io",
        "outputId": "17a9faf6-6a0f-47f1-ab6c-6bd041d3602c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTc-FmPXr3bp",
        "outputId": "da8ea1de-03fc-43ed-b8b8-bc12536d62ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/drive/MyDrive/FYP_Models/Image_Captioning/resnet50_model.pth\"\n",
        "VOCAB_PATH = \"/content/drive/MyDrive/FYP_Models/Image_Captioning/vocab.pt\""
      ],
      "metadata": {
        "id": "iqqw_h-_r5bh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx, idx2word = torch.load(VOCAB_PATH)\n",
        "vocab_size = len(word2idx)\n",
        "print(\"‚úÖ Vocabulary loaded:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXj_uhyKr_jv",
        "outputId": "6bb77b5b-6b05-4b9a-c4e2-8f8b9e5e826b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vocabulary loaded: 3004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size=2048, embed_size=256, hidden_size=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.feature_fc = nn.Linear(feature_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        h0 = self.feature_fc(features).unsqueeze(0).unsqueeze(0)\n",
        "        c0 = torch.zeros_like(h0)\n",
        "        emb = self.embedding(captions)\n",
        "        out, _ = self.lstm(emb, (h0, c0))\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "vOd3GHxxsCD5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CaptionModel(vocab_size).to(device)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Caption model loaded from Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_oWFevwsDBf",
        "outputId": "423cc833-c724-4fd2-debd-c7c179f1e856"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Caption model loaded from Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "resnet.eval().to(device)\n",
        "\n",
        "for p in resnet.parameters():\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkMTDGL1sFYy",
        "outputId": "0b92c3f3-1a24-4f28-afa2-a3f20931ccda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 159MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "qlTpT7s4sLlF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OBJ_EMBED = nn.Embedding(1000, 256).to(device)\n",
        "fusion_fc = nn.Linear(2048 + 256, 2048).to(device)\n",
        "\n",
        "def fuse_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_t = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # ResNet features\n",
        "    with torch.no_grad():\n",
        "        visual_feat = resnet(img_t).squeeze()\n",
        "\n",
        "    return visual_feat # Return only visual_feat"
      ],
      "metadata": {
        "id": "CLr5tqpAsN7W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_caption(feature, max_len=20, temperature=0.8):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h = model.feature_fc(feature).unsqueeze(0).unsqueeze(0)\n",
        "        c = torch.zeros_like(h)\n",
        "\n",
        "        # Use the SAME tokenization as training\n",
        "        start_tokens = [\n",
        "            word2idx['<'],\n",
        "            word2idx['start'],\n",
        "            word2idx['>']\n",
        "        ]\n",
        "\n",
        "        word = torch.tensor([start_tokens]).to(device)\n",
        "        result = []\n",
        "\n",
        "        for step in range(max_len):\n",
        "            emb = model.embedding(word[:, -1:])\n",
        "            out, (h, c) = model.lstm(emb, (h, c))\n",
        "            scores = model.fc(out.squeeze(1))\n",
        "\n",
        "            probs = F.softmax(scores / temperature, dim=1)\n",
        "            pred = torch.multinomial(probs, 1).item()\n",
        "            predicted_word = idx2word[pred]\n",
        "\n",
        "            # stop condition\n",
        "            if predicted_word == 'end':\n",
        "                break\n",
        "\n",
        "            # avoid printing special tokens\n",
        "            if predicted_word not in {'<', 'start', '>'}:\n",
        "                result.append(predicted_word)\n",
        "\n",
        "            word = torch.cat(\n",
        "                [word, torch.tensor([[pred]]).to(device)],\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "        return \" \".join(result)"
      ],
      "metadata": {
        "id": "1lyUAiJANJGW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "srC9m-9BRbEI",
        "outputId": "284b1384-2efe-498a-cd93-4e78fefe59a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e70f167-8edf-478d-8ea7-bec456c3c569\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e70f167-8edf-478d-8ea7-bec456c3c569\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dog.png to dog.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BEAM SEARCH**"
      ],
      "metadata": {
        "id": "knbAIj9ybbjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_caption_beam(feature, beam_size=3, max_len=20):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h = model.feature_fc(feature).unsqueeze(0).unsqueeze(0)\n",
        "        c = torch.zeros_like(h)\n",
        "\n",
        "        # (sequence, score, h, c)\n",
        "        # The sequence here will be a list of token IDs\n",
        "        start_tokens = [word2idx['<'], word2idx['start'], word2idx['>']]\n",
        "        beams = [(start_tokens, 0.0, h, c)] # (sequence_of_ids, log_prob_score, hidden_state, cell_state)\n",
        "\n",
        "        # To store completed captions\n",
        "        final_captions = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            if not beams: # All beams have completed\n",
        "                break\n",
        "\n",
        "            new_beams = []\n",
        "            for seq, score, h_state, c_state in beams:\n",
        "                # If this beam already ended (from previous step), just add it to final_captions and don't extend\n",
        "                if idx2word[seq[-1]] == 'end':\n",
        "                    final_captions.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                last_word_tensor = torch.tensor([[seq[-1]]]).to(device)\n",
        "                emb = model.embedding(last_word_tensor)\n",
        "                out, (h_next, c_next) = model.lstm(emb, (h_state, c_state))\n",
        "                logits = model.fc(out.squeeze(1))\n",
        "                probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                topk_probs, topk_idx = probs.topk(beam_size)\n",
        "\n",
        "                for i in range(beam_size):\n",
        "                    next_word_idx = topk_idx[0][i].item()\n",
        "                    new_score = score + topk_probs[0][i].item()\n",
        "                    new_seq = seq + [next_word_idx]\n",
        "\n",
        "                    if idx2word[next_word_idx] == 'end':\n",
        "                        final_captions.append((new_seq, new_score))\n",
        "                    else:\n",
        "                        new_beams.append((new_seq, new_score, h_next, c_next))\n",
        "\n",
        "            # Select the top beam_size beams for the next iteration (prioritizing higher scores)\n",
        "            # This step keeps only active beams and sorts them for the next iteration\n",
        "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "        # After the loop, if there are any remaining active beams, treat them as complete\n",
        "        final_captions.extend([(seq, score) for seq, score, _, _ in beams])\n",
        "\n",
        "        if not final_captions:\n",
        "            # Fallback if no captions were completed (this shouldn't happen with reasonable max_len)\n",
        "            # If this occurs, it indicates an issue, so we'll return a placeholder string.\n",
        "            return \"Generation failed to complete.\"\n",
        "\n",
        "        # Sort all completed captions (including those that hit max_len) and pick the best one\n",
        "        best_seq_with_score = sorted(final_captions, key=lambda x: x[1], reverse=True)[0]\n",
        "        best_seq = best_seq_with_score[0]\n",
        "\n",
        "        caption_words = []\n",
        "        for idx in best_seq:\n",
        "            word = idx2word[idx]\n",
        "            if word in {'<', 'start', '>', 'end'}:\n",
        "                continue\n",
        "            caption_words.append(word)\n",
        "\n",
        "        return \" \".join(caption_words)"
      ],
      "metadata": {
        "id": "YRxXhMM0bU61"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature = fuse_features(image_path)\n",
        "print(\"üñºÔ∏è Caption (Beam):\", generate_caption_beam(feature))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPZULdh-bY8F",
        "outputId": "c525a2cd-40ed-49d6-cb9c-4793e2b5379c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Caption (Beam): a brown and white dog is running on the grass .\n"
          ]
        }
      ]
    }
  ]
}