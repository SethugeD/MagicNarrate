{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0z0E8GL1Sxr",
        "outputId": "7a710c90-bb32-42ef-c316-3b70ef0b1475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision nltk tqdm kagglehub"
      ],
      "metadata": {
        "id": "C52t7xCs1fQ4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6yNV4RK2nG3",
        "outputId": "38f7b062-cc4c-40b1-9bec-ce53416503e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/FYP_Models/Image_Captioning\"\n",
        "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
        "\n",
        "MODEL_PATH = os.path.join(DRIVE_ROOT, \"resnet50_model.pth\")\n",
        "VOCAB_PATH = os.path.join(DRIVE_ROOT, \"vocab.pt\")\n",
        "FEATURE_PATH = os.path.join(DRIVE_ROOT, \"resnet50_features.pt\")\n",
        "\n",
        "print(\"Drive directory ready:\", DRIVE_ROOT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTyW4SHl2q2j",
        "outputId": "d03079ea-4931-46ab-b133-4e820fe6a651"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive directory ready: /content/drive/MyDrive/FYP_Models/Image_Captioning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "print(\"Dataset path:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxHsSrfu1hyy",
        "outputId": "f8cca28d-6e48-467d-bc64-6d910a1f737b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'flickr8k' dataset.\n",
            "Dataset path: /kaggle/input/flickr8k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Corrected paths based on Kaggle dataset structure\n",
        "IMAGE_DIR = os.path.join(path, \"Images\")\n",
        "CAPTION_FILE = os.path.join(path, \"captions.txt\")\n",
        "\n",
        "print(len(os.listdir(IMAGE_DIR)), \"images found\")\n",
        "print(\"Caption file exists:\", os.path.exists(CAPTION_FILE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q09oNiF1izA",
        "outputId": "b4d613fd-beb7-45eb-912e-a6101fa1412b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8091 images found\n",
            "Caption file exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "eFY5VXMP1lM5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gir3RAYR1nbX",
        "outputId": "ba6a9459-f74f-4363-c11f-b614cf9ce4bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv # Import the csv module to handle quoted fields\n",
        "captions = {}\n",
        "\n",
        "with open(CAPTION_FILE, \"r\") as f:\n",
        "    reader = csv.reader(f) # Use csv.reader to parse the file\n",
        "    next(reader) # Skip the header line\n",
        "    for row in reader:\n",
        "        if len(row) >= 2: # Ensure the row has at least two columns (image, caption)\n",
        "            img_name = row[0] # Image name is directly in the first column\n",
        "            caption_text = row[1] # Caption text is in the second column\n",
        "\n",
        "            caption_text = \"<start> \" + caption_text.lower() + \" <end>\"\n",
        "            captions.setdefault(img_name, []).append(caption_text)\n",
        "        else:\n",
        "            # Optionally, log or handle malformed rows if necessary\n",
        "            print(f\"Skipping malformed row: {row}\")\n",
        "\n",
        "print(\"Total images with captions:\", len(captions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7lk22581qWR",
        "outputId": "fbb3d882-7540-40a4-a26e-3d8ad4a2041b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images with captions: 8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "word_counter = Counter()\n",
        "\n",
        "for caps in captions.values():\n",
        "    for c in caps:\n",
        "        word_counter.update(word_tokenize(c))\n",
        "\n",
        "vocab = [w for w, c in word_counter.items() if c >= 5]\n",
        "\n",
        "word2idx = {w: i+1 for i, w in enumerate(vocab)}\n",
        "word2idx[\"<pad>\"] = 0\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# Save vocabulary to Google Drive\n",
        "torch.save((word2idx, idx2word), VOCAB_PATH)\n",
        "print(\"Vocabulary saved to Drive:\", VOCAB_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmRNAcLa1sEP",
        "outputId": "6662da6a-5619-42d9-8c12-975c73922f8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3004\n",
            "Vocabulary saved to Drive: /content/drive/MyDrive/FYP_Models/Image_Captioning/vocab.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "_hlOQXzr1uRi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "resnet.eval()\n",
        "\n",
        "for p in resnet.parameters():\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5_1GQkX1wyV",
        "outputId": "92b21de2-d98b-4156-87e1-597ac98ccee4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 171MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(FEATURE_PATH):\n",
        "    print(\"Loading image features from Drive...\")\n",
        "    features = torch.load(FEATURE_PATH)\n",
        "else:\n",
        "    print(\"Extracting image features (one-time)...\")\n",
        "    features = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_name in tqdm(captions.keys()):\n",
        "            img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            image = transform(image).unsqueeze(0)\n",
        "            feature = feature = resnet(image).squeeze().detach()\n",
        "            features[img_name] = feature\n",
        "\n",
        "    torch.save(features, FEATURE_PATH)\n",
        "    print(\"Image features saved to Drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weKwf_p-1ywW",
        "outputId": "03f1905d-ac92-444d-dd06-3c2dbd4fa07f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading image features from Drive...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_to_seq(caption):\n",
        "    tokens = word_tokenize(caption)\n",
        "    return [word2idx.get(w, 0) for w in tokens]"
      ],
      "metadata": {
        "id": "y5KcSveV10_7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size=2048, embed_size=256, hidden_size=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.feature_fc = nn.Linear(feature_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        # features: [2048]\n",
        "        h0 = self.feature_fc(features).unsqueeze(0).unsqueeze(0)\n",
        "        c0 = torch.zeros_like(h0)\n",
        "\n",
        "        embeddings = self.embedding(captions)\n",
        "        outputs, _ = self.lstm(embeddings, (h0, c0))\n",
        "        outputs = self.fc(outputs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "0SoCvuHI13gl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CaptionModel(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "jhdDdmrp15nR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    step_count = 0\n",
        "\n",
        "    for img, caps in captions.items():\n",
        "        feature = features[img].to(device)\n",
        "\n",
        "        for c in caps:\n",
        "            seq = torch.tensor(caption_to_seq(c)).unsqueeze(0).to(device)\n",
        "            inputs = seq[:, :-1]\n",
        "            targets = seq[:, 1:]\n",
        "\n",
        "            outputs = model(feature, inputs)\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, vocab_size),\n",
        "                targets.reshape(-1)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            step_count += 1\n",
        "\n",
        "    avg_loss = total_loss / step_count\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faxPt06f17i2",
        "outputId": "4c35b3ea-5066-47b6-d4e1-1e4f513afeb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Avg Loss: 2.4964\n",
            "Epoch 2/50 | Avg Loss: 2.1198\n",
            "Epoch 3/50 | Avg Loss: 1.9782\n",
            "Epoch 4/50 | Avg Loss: 1.8790\n",
            "Epoch 5/50 | Avg Loss: 1.7965\n",
            "Epoch 6/50 | Avg Loss: 1.7239\n",
            "Epoch 7/50 | Avg Loss: 1.6573\n",
            "Epoch 8/50 | Avg Loss: 1.5955\n",
            "Epoch 9/50 | Avg Loss: 1.5368\n",
            "Epoch 10/50 | Avg Loss: 1.4816\n",
            "Epoch 11/50 | Avg Loss: 1.4283\n",
            "Epoch 12/50 | Avg Loss: 1.3796\n",
            "Epoch 13/50 | Avg Loss: 1.3341\n",
            "Epoch 14/50 | Avg Loss: 1.2913\n",
            "Epoch 15/50 | Avg Loss: 1.2534\n",
            "Epoch 16/50 | Avg Loss: 1.2146\n",
            "Epoch 17/50 | Avg Loss: 1.1804\n",
            "Epoch 18/50 | Avg Loss: 1.1489\n",
            "Epoch 19/50 | Avg Loss: 1.1175\n",
            "Epoch 20/50 | Avg Loss: 1.0927\n",
            "Epoch 21/50 | Avg Loss: 1.0666\n",
            "Epoch 22/50 | Avg Loss: 1.0419\n",
            "Epoch 23/50 | Avg Loss: 1.0184\n",
            "Epoch 24/50 | Avg Loss: 0.9992\n",
            "Epoch 25/50 | Avg Loss: 0.9782\n",
            "Epoch 26/50 | Avg Loss: 0.9595\n",
            "Epoch 27/50 | Avg Loss: 0.9461\n",
            "Epoch 28/50 | Avg Loss: 0.9330\n",
            "Epoch 29/50 | Avg Loss: 0.9170\n",
            "Epoch 30/50 | Avg Loss: 0.9038\n",
            "Epoch 31/50 | Avg Loss: 0.8897\n",
            "Epoch 32/50 | Avg Loss: 0.8747\n",
            "Epoch 33/50 | Avg Loss: 0.8659\n",
            "Epoch 34/50 | Avg Loss: 0.8532\n",
            "Epoch 35/50 | Avg Loss: 0.8401\n",
            "Epoch 36/50 | Avg Loss: 0.8303\n",
            "Epoch 37/50 | Avg Loss: 0.8201\n",
            "Epoch 38/50 | Avg Loss: 0.8087\n",
            "Epoch 39/50 | Avg Loss: 0.7984\n",
            "Epoch 40/50 | Avg Loss: 0.7932\n",
            "Epoch 41/50 | Avg Loss: 0.7858\n",
            "Epoch 42/50 | Avg Loss: 0.7779\n",
            "Epoch 43/50 | Avg Loss: 0.7681\n",
            "Epoch 44/50 | Avg Loss: 0.7609\n",
            "Epoch 45/50 | Avg Loss: 0.7576\n",
            "Epoch 46/50 | Avg Loss: 0.7516\n",
            "Epoch 47/50 | Avg Loss: 0.7494\n",
            "Epoch 48/50 | Avg Loss: 0.7437\n",
            "Epoch 49/50 | Avg Loss: 0.7395\n",
            "Epoch 50/50 | Avg Loss: 0.7345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(\"Model saved to Google Drive:\", MODEL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQLuHBv41_9A",
        "outputId": "abc54764-8021-4af8-e00e-97f2dfcc175d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to Google Drive: /content/drive/MyDrive/FYP_Models/Image_Captioning/resnet50_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded from Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBV9jebV2Ay3",
        "outputId": "f8822ca6-65ac-4ebf-c039-08dfef6ca5f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from Drive\n"
          ]
        }
      ]
    }
  ]
}